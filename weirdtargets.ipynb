{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"WeirdTargets Module\"\"\"\n",
    "##########################################\n",
    "#      Future Statement Definitions      #\n",
    "##########################################\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import generator_stop\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import absolute_import\n",
    "##########################################\n",
    "#        Python Standard Library         #\n",
    "##########################################\n",
    "import argparse\n",
    "import re \n",
    "import math\n",
    "import json \n",
    "import itertools\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import struct\n",
    "import datetime\n",
    "import inspect\n",
    "import collections\n",
    "import pprint\n",
    "# import multiprocessing\n",
    "import operator as op\n",
    "from functools import reduce\n",
    "import requests\n",
    "from copy import copy, deepcopy\n",
    "##########################################\n",
    "#   3rd Party Data Loading & Analysis    #\n",
    "##########################################\n",
    "import ujson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from gzip import GzipFile\n",
    "from toolz import partition_all\n",
    "##########################################\n",
    "#      3rd Party Parallel Computing      #\n",
    "##########################################\n",
    "import pathos\n",
    "import psutil\n",
    "import dask.dataframe as dd\n",
    "\"\"\"import pathos.multiprocessing as pmp\n",
    "import pathos.pools as ppools\n",
    "import pathos.pp as ppp\"\"\"\n",
    "import dask.dataframe as dd\n",
    "\"\"\"from dask.threaded import get as ddscheduler\"\"\"\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "from opentargets import OpenTargetsClient\n",
    "##########################################\n",
    "#           Ignore Warnings              #\n",
    "##########################################\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "##########################################\n",
    "#       Module Level Dunder Names        #\n",
    "##########################################\n",
    "__copyright__ = \"Copyrights © 2019 Aly shmahell.\"\n",
    "__credits__   = [\"Aly Shmahell\"]\n",
    "__version__   = \"0.1.1\"\n",
    "__maintainer__= \"Aly Shmahell\"\n",
    "__email__     = [\"aly.shmahell@gmail.com\"]\n",
    "__status__    = \"Alpha\"\n",
    "##########################################\n",
    "##########################################\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nCr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncr(n, r):\n",
    "    r = min(r, n-r)\n",
    "    numer = reduce(op.mul, range(n, n-r, -1), 1)\n",
    "    denom = reduce(op.mul, range(1, r+1), 1)\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloader(url, filepath):\n",
    "    r = requests.get(url, stream=True)\n",
    "    total_size = int(r.headers.get('content-length', 0))\n",
    "    block_size = 1024**2\n",
    "    t=tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "    with open(filepath, 'wb') as f:\n",
    "        for data in r.iter_content(block_size):\n",
    "            t.update(len(data))\n",
    "            f.write(data)\n",
    "    t.close()\n",
    "    if total_size != 0 and t.n != total_size:\n",
    "        print(\"ERROR, something went wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exception Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeirdTargetsException(Exception):\n",
    "    \"\"\"\n",
    "    Exception Class\n",
    "    \"\"\"\n",
    "    __module__ = Exception.__module__\n",
    "    def __init__(self, error):\n",
    "        try:\n",
    "            line = sys.exc_info()[-1].tb_lineno\n",
    "        except AttributeError:\n",
    "            line = inspect.currentframe().f_back.f_lineno\n",
    "        self.args = f\"{type(self).__name__} (line {line}): {error}\",\n",
    "        sys.exit(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeirdTargetsPrinter(object):\n",
    "  \n",
    "    def pretty(self, x):\n",
    "        return re.sub(r\"\\n\\s+\", \"\\n\", x)\n",
    "\n",
    "    def oneliner(self, x):\n",
    "        return re.sub(r\"\\n\\s*\", \" \", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WeirdTagets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeirdTargets(WeirdTargetsPrinter):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.empty_string = \"\"\n",
    "        \n",
    "    \"\"\"def testParallelism(self):\n",
    "        print(\"Testing Parallelism:\")\n",
    "        print(f\"\\t No. of Cores Available: {pmp.cpu_count()}\")\n",
    "        with pmp.Pool(pmp.cpu_count()) as pool:\n",
    "            PIDS = pool.map(lambda _: f\"{os.getpid()}\", range(pmp.cpu_count()+1))\n",
    "            print(f\"\\t No. of Cores Utilized:  {np.unique(PIDS).size}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weird_targets.testParallelism()'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weird_targets = WeirdTargets()\n",
    "\"\"\"weird_targets.testParallelism()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmallTargets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class SmallTargets(WeirdTargets):\\n\\n    def __init__(self, type, id):\\n        self.type         = type\\n        self.id           = id\\n        self.inputs       = None\\n        self.outputs      = []\\n        self.elapsed_time = None\\n        super(SmallTargets, self).__init__()\\n\\n    def __call__(self):\\n        if self.type == \"disease\":\\n            try:\\n                self.inputs = OpenTargetsClient().get_associations_for_disease(self.id)\\n            except:\\n                raise WeirdTargetsException(f\"Incorrect Disease ID: {self.id}\")\\n        if self.type == \"target\":\\n            try:\\n                self.inputs = OpenTargetsClient().get_associations_for_target(self.id)\\n            except:\\n                raise WeirdTargetsException(f\"Incorrect Target ID: {self.id}\")\\n        if not self.inputs:\\n            raise WeirdTargetsException(self.oneliner(\"\"\"The query did not\\n                                                          return any usefull\\n                                                          information.\"\"\"))\\n        self.elapsed_time = time.time()\\n        with pmp.Pool(pmp.cpu_count()) as pool:\\n            overalls           = pool.map(lambda entry: entry[\\'association_score\\'][\\'overall\\'], self.inputs)\\n            squared_overalls   = pool.map(lambda overall: overall**2,                          overalls)\\n            minimum            = min(overalls)\\n            maximum            = max(overalls)\\n            average            = sum(overalls)/len(self.inputs)\\n            standard_deviation = np.sqrt(\\n                sum(squared_overalls)/len(self.inputs) - average**2)\\n            self.outputs = {\\n                \"Maximum\"           : maximum,\\n                \"Minimum\"           : minimum,\\n                \"Average\"           : average,\\n                \"Standard Deviation\": standard_deviation\\n            }\\n        self.elapsed_time = time.time() - self.elapsed_time\\n\\n    def __str__(self):\\n        if not self.outputs:\\n            raise WeirdTargetsException(self.oneliner(\"\"\"you need to call \\n                                                          the SmallTargets \\n                                                          object first.\"\"\"))\\n        return self.pretty(f\"\"\"Number of Entries :       {len(self.inputs)}\\n\\n                                Elapsed Time      :       {self.elapsed_time} sec\\n\\n                                Maximum           :       {self.outputs[\\'Maximum\\']}\\n\\n                                Minumum           :       {self.outputs[\\'Minimum\\']}\\n\\n                                Average           :       {self.outputs[\\'Average\\']}\\n\\n                                Standard Deviation:       {self.outputs[\\'Standard Deviation\\']}\"\"\")'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class SmallTargets(WeirdTargets):\n",
    "\n",
    "    def __init__(self, type, id):\n",
    "        self.type         = type\n",
    "        self.id           = id\n",
    "        self.inputs       = None\n",
    "        self.outputs      = []\n",
    "        self.elapsed_time = None\n",
    "        super(SmallTargets, self).__init__()\n",
    "\n",
    "    def __call__(self):\n",
    "        if self.type == \"disease\":\n",
    "            try:\n",
    "                self.inputs = OpenTargetsClient().get_associations_for_disease(self.id)\n",
    "            except:\n",
    "                raise WeirdTargetsException(f\"Incorrect Disease ID: {self.id}\")\n",
    "        if self.type == \"target\":\n",
    "            try:\n",
    "                self.inputs = OpenTargetsClient().get_associations_for_target(self.id)\n",
    "            except:\n",
    "                raise WeirdTargetsException(f\"Incorrect Target ID: {self.id}\")\n",
    "        if not self.inputs:\n",
    "            raise WeirdTargetsException(self.oneliner(\"\"\"The query did not\n",
    "                                                          return any usefull\n",
    "                                                          information.\"\"\"))\n",
    "        self.elapsed_time = time.time()\n",
    "        with pmp.Pool(pmp.cpu_count()) as pool:\n",
    "            overalls           = pool.map(lambda entry: entry['association_score']['overall'], self.inputs)\n",
    "            squared_overalls   = pool.map(lambda overall: overall**2,                          overalls)\n",
    "            minimum            = min(overalls)\n",
    "            maximum            = max(overalls)\n",
    "            average            = sum(overalls)/len(self.inputs)\n",
    "            standard_deviation = np.sqrt(\n",
    "                sum(squared_overalls)/len(self.inputs) - average**2)\n",
    "            self.outputs = {\n",
    "                \"Maximum\"           : maximum,\n",
    "                \"Minimum\"           : minimum,\n",
    "                \"Average\"           : average,\n",
    "                \"Standard Deviation\": standard_deviation\n",
    "            }\n",
    "        self.elapsed_time = time.time() - self.elapsed_time\n",
    "\n",
    "    def __str__(self):\n",
    "        if not self.outputs:\n",
    "            raise WeirdTargetsException(self.oneliner(\"\"\"you need to call \n",
    "                                                          the SmallTargets \n",
    "                                                          object first.\"\"\"))\n",
    "        return self.pretty(f\"\"\"Number of Entries :       {len(self.inputs)}\\n\n",
    "                                Elapsed Time      :       {self.elapsed_time} sec\\n\n",
    "                                Maximum           :       {self.outputs['Maximum']}\\n\n",
    "                                Minumum           :       {self.outputs['Minimum']}\\n\n",
    "                                Average           :       {self.outputs['Average']}\\n\n",
    "                                Standard Deviation:       {self.outputs['Standard Deviation']}\"\"\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"small_targets = SmallTargets('target', 'ENSG00000157764')\\nsmall_targets()\\nprint(small_targets)\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"small_targets = SmallTargets('target', 'ENSG00000157764')\n",
    "small_targets()\n",
    "print(small_targets)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigTargets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class BigTargets(WeirdTargets):\n",
    "\n",
    "    def __init__(self, \n",
    "                 rawdir, \n",
    "                 rawurl, \n",
    "                 keysdict, \n",
    "                 outdir, \n",
    "                 outfile, \n",
    "                 groupnames, \n",
    "                 resultdir, \n",
    "                 resultfile, \n",
    "                 saveresults=False):\n",
    "        if not os.path.isdir(outdir):\n",
    "            os.mkdir(outdir, 755)\n",
    "        if not os.path.isdir(resultdir):\n",
    "            os.mkdir(resultdir, 755)\n",
    "        if not os.path.isdir(rawdir):\n",
    "            os.mkdir(rawdir, 755)\n",
    "        super(BigTargets, self).__init__()\n",
    "        self.rawdir         :\"rawdir\"      = rawdir\n",
    "        self.rawfile        :\"rawfile\"     = rawurl.split('/')[-1]\n",
    "        self.rawurl         :\"rawurl\"      = rawurl\n",
    "        self.infile         :\"infile\"      = os.path.join(self.rawdir, self.rawfile)\n",
    "        self.keysdict       :\"keysdict\"    = keysdict\n",
    "        self.outdir         :\"outdir\"      = outdir\n",
    "        self.outfile        :\"outfile\"     = outfile\n",
    "        self.groupnames     :\"groupnames\"  = groupnames\n",
    "        self.resultdir      :\"resultdir\"   = resultdir\n",
    "        self.resultfile     :\"resultfile\"  = resultfile\n",
    "        self.saveresults    :\"saveresults\" = saveresults\n",
    "\n",
    "    def __download__(self):\n",
    "        downloader(self.rawurl, self.infile)\n",
    "\n",
    "    def __traversejson__(self, parsed, keys):\n",
    "        if len(keys)>1:\n",
    "            return self.__traverse__(parsed[keys[0]], keys[1:])\n",
    "        return parsed[keys[0]]\n",
    "\n",
    "    def __parsejson__(self, pyObject, tqdmObject):\n",
    "        tqdmObject.update(1)\n",
    "        parsed = ujson.loads(pyObject)\n",
    "        obj = {\n",
    "            key: self.__traversejson__(parsed, self.keysdict[key])\n",
    "            for key in self.keysdict.keys()\n",
    "        }\n",
    "        return obj\n",
    "\n",
    "    def __json2panda__(self, batch, tqdmObject):\n",
    "        parsedJSON = map(lambda b: self.__parsejson__(b, tqdmObject), batch)\n",
    "        df = pd.DataFrame.from_records(parsedJSON, \n",
    "                                       columns=[\"target\",\n",
    "                                                \"disease\",\n",
    "                                                \"score\"])\n",
    "        return df\n",
    "    def __peek__(self, iterable):\n",
    "        try:\n",
    "            first = next(iterable)\n",
    "        except StopIteration:\n",
    "            return None, None\n",
    "        return first, itertools.chain([first], iterable)\n",
    "\n",
    "    def __persist__(self):\n",
    "        with tqdm(desc=f\"Feature Extraction{self.empty_string:>18}\") as tqdmObject:\n",
    "            with GzipFile(self.infile) as f:\n",
    "                batches = partition_all(\n",
    "                    math.floor(psutil.virtual_memory()[1]/(1024**3))*int(2e+4), f\n",
    "                )\n",
    "                while True:\n",
    "                    df, frames = self.__peek__(\n",
    "                        map(\n",
    "                            lambda b: self.__json2panda__(b, tqdmObject), batches\n",
    "                        )\n",
    "                    )\n",
    "                    if frames == None:\n",
    "                        break\n",
    "                    df.to_hdf(\n",
    "                        os.path.join(self.outdir, self.outfile),\n",
    "                        key=f'{self.outfile.split(\".\")[0]}',\n",
    "                        mode='a',\n",
    "                        format='table',\n",
    "                        append=True\n",
    "                    )\n",
    "\n",
    "    def __process__(self):\n",
    "        with tqdm(desc=f\"Mapping{self.empty_string:>18}\") as tqdmo:\n",
    "            def setter(x):\n",
    "                tqdmo.update(1)\n",
    "                return set(x.to_numpy())\n",
    "            groups = self.df.groupby(self.groupnames[0])[self.groupnames[1]].apply(setter)\n",
    "        array = groups.to_numpy()\n",
    "        names = list(groups.keys())\n",
    "        combinations = itertools.combinations(range(len(groups)), 2)\n",
    "        nck = int(ncr(array.shape[0], 2))\n",
    "        if self.saveresults:\n",
    "            temp = np.memmap(\n",
    "                os.path.join(self.resultdir, 'temp.memmap'), \n",
    "                dtype='|U30', \n",
    "                mode='w+', \n",
    "                shape=(nck,2)\n",
    "            )\n",
    "        counter = 0\n",
    "        index   = 0\n",
    "        with tqdm(desc=f\"Reducing{self.empty_string:>17}\", iterable=range(nck)) as tqdmo:\n",
    "            def __compare__(combination):\n",
    "                common = array[combination[0]] & array[combination[1]]\n",
    "                if len(common) >= 2:\n",
    "                    return 1, \", \".join([names[combination[0]], names[combination[1]]])\n",
    "                else:\n",
    "                    return 0, None\n",
    "            for combination in combinations:\n",
    "                partial_result = __compare__(combination)\n",
    "                counter += partial_result[0]\n",
    "                tqdmo.update(1)\n",
    "                if partial_result[0] > 0 and self.saveresults:\n",
    "                    temp[index][0] = partial_result[0]\n",
    "                    temp[index][1] = partial_result[1]\n",
    "                    index += 1\n",
    "        if self.saveresults:\n",
    "            results = np.memmap(\n",
    "                os.path.join(self.resultdir, self.resultfile), \n",
    "                dtype='|U30', \n",
    "                mode='w+', \n",
    "                shape=(trueindex, 2)\n",
    "            )\n",
    "            results[:] = temp[:trueindex]\n",
    "            os.remove(os.path.join(self.resultdir, 'temp.memmap'))\n",
    "        else:\n",
    "            results = None\n",
    "        return results, counter\n",
    "    def __call__(self):\n",
    "        try:\n",
    "            if not (os.path.exists(os.path.join(self.rawdir, self.rawfile))):\n",
    "                self.__download__()\n",
    "            if not (os.path.exists(os.path.join(self.outdir, self.outfile))):\n",
    "                self.__persist__()\n",
    "            print(\"Loading HD5\")\n",
    "            self.df     = dd.read_hdf(\n",
    "                os.path.join(self.outdir, self.outfile),\n",
    "                key=f'{self.outfile.split(\".\")[0]}'\n",
    "            ).compute()\n",
    "            self.df = self.df.head(10000)\n",
    "            results, counter = self.__process__()\n",
    "            print(f\"No. Complete Bipartites{self.empty_string:>1}: {counter}\")\n",
    "        except KeyboardInterrupt:\n",
    "            for name in dir():\n",
    "                if not name.startswith('_'):\n",
    "                    if name in globals():\n",
    "                        del globals()[name]\n",
    "                    else:\n",
    "                        del locals()[name]\n",
    "            gc.collect()\n",
    "            WeirdTargetsException(\"Keyboard Interrupt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HD5\n",
      "[########################################] | 100% Completed |  9.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping                  : 2989it [00:00, 6730.45it/s]\n",
      "Reducing                 : 100%|██████████| 4465566/4465566 [00:04<00:00, 929839.97it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. Complete Bipartites : 29251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "big_targets = BigTargets(\n",
    "    './content/datasets',\n",
    "    'https://storage.googleapis.com/open-targets-data-releases/17.12/17.12_evidence_data.json.gz',\n",
    "    {\n",
    "        \"target\" : [\"target\", \"id\"],\n",
    "        \"disease\": [\"disease\", \"id\"],\n",
    "        \"score\":   [\"scores\", \"association_score\"]\n",
    "    }, \n",
    "    './content/results',\n",
    "    'tds.h5',\n",
    "    ['target', 'disease'],\n",
    "    './content/results',\n",
    "    'result.memmap'\n",
    ")\n",
    "big_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigTargets2(WeirdTargets):\n",
    "\n",
    "    def __init__(self, \n",
    "                 rawdir, \n",
    "                 rawurl, \n",
    "                 keysdict, \n",
    "                 outdir, \n",
    "                 outfile, \n",
    "                 groupnames, \n",
    "                 resultdir, \n",
    "                 resultfile, \n",
    "                 saveresults=False):\n",
    "        if not os.path.isdir(outdir):\n",
    "            os.mkdir(outdir, 755)\n",
    "        if not os.path.isdir(resultdir):\n",
    "            os.mkdir(resultdir, 755)\n",
    "        if not os.path.isdir(rawdir):\n",
    "            os.mkdir(rawdir, 755)\n",
    "        super(BigTargets2, self).__init__()\n",
    "        self.rawdir         :\"rawdir\"      = rawdir\n",
    "        self.rawfile        :\"rawfile\"     = rawurl.split('/')[-1]\n",
    "        self.rawurl         :\"rawurl\"      = rawurl\n",
    "        self.infile         :\"infile\"      = os.path.join(self.rawdir, self.rawfile)\n",
    "        self.keysdict       :\"keysdict\"    = keysdict\n",
    "        self.outdir         :\"outdir\"      = outdir\n",
    "        self.outfile        :\"outfile\"     = outfile\n",
    "        self.groupnames     :\"groupnames\"  = groupnames\n",
    "        self.resultdir      :\"resultdir\"   = resultdir\n",
    "        self.resultfile     :\"resultfile\"  = resultfile\n",
    "\n",
    "    def __download__(self):\n",
    "        downloader(self.rawurl, self.infile)\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    def __traversejson__(self, parsed, keys):\n",
    "        if len(keys)>1:\n",
    "            return self.__traverse__(parsed[keys[0]], keys[1:])\n",
    "        return parsed[keys[0]]\n",
    "\n",
    "    def __parsejson__(self, pyObject, tqdmObject):\n",
    "        tqdmObject.update(1)\n",
    "        parsed = ujson.loads(pyObject)\n",
    "        obj = {\n",
    "            key: self.__traversejson__(parsed, self.keysdict[key])\n",
    "            for key in self.keysdict.keys()\n",
    "        }\n",
    "        return obj\n",
    "\n",
    "    def __json2panda__(self, batch, tqdmObject):\n",
    "        parsedJSON = map(lambda b: self.__parsejson__(b, tqdmObject), batch)\n",
    "        df = pd.DataFrame.from_records(parsedJSON, \n",
    "                                       columns=[\"target\",\n",
    "                                                \"disease\",\n",
    "                                                \"score\"])\n",
    "        return df\n",
    "    def __peek__(self, iterable):\n",
    "        try:\n",
    "            first = next(iterable)\n",
    "        except StopIteration:\n",
    "            return None, None\n",
    "        return first, itertools.chain([first], iterable)\n",
    "\n",
    "    def __persist__(self):\n",
    "        with tqdm(desc=f\"Feature Extraction{self.empty_string:>18}\") as tqdmObject:\n",
    "            with GzipFile(self.infile) as f:\n",
    "                batches = partition_all(\n",
    "                    math.floor(psutil.virtual_memory()[1]/(1024**3))*int(2e+4), f\n",
    "                )\n",
    "                while True:\n",
    "                    df, frames = self.__peek__(\n",
    "                        map(\n",
    "                            lambda b: self.__json2panda__(b, tqdmObject), batches\n",
    "                        )\n",
    "                    )\n",
    "                    if frames == None:\n",
    "                        break\n",
    "                    df.to_hdf(\n",
    "                        os.path.join(self.outdir, self.outfile),\n",
    "                        key=f'{self.outfile.split(\".\")[0]}',\n",
    "                        mode='a',\n",
    "                        format='table',\n",
    "                        append=True\n",
    "                    )\n",
    "                    \n",
    "    def __process__(self):\n",
    "        with tqdm(desc=f\"Mapping{self.empty_string:>18}\") as tqdmo:\n",
    "            def setter(x):\n",
    "                tqdmo.update(1)\n",
    "                return set(x.to_numpy())\n",
    "            groups = self.df.groupby(self.groupnames[0])[self.groupnames[1]].apply(setter)\n",
    "        self.array = groups.to_numpy()\n",
    "        self.names = names = list(groups.keys())\n",
    "        self.combinations = itertools.combinations(range(self.array.shape[0]), 2)\n",
    "        nck = int(ncr(self.array.shape[0], 2))\n",
    "        self.counter = 0\n",
    "        self.index   = 0\n",
    "        cpus = 8\n",
    "        base = int(nck/cpus)\n",
    "        self.chunks = []\n",
    "        cc = 0\n",
    "        partition = 0\n",
    "        for successive in range(base, nck, base):\n",
    "            self.chunks.append(\n",
    "                            {\n",
    "                                \"num\": cc,\n",
    "                                \"len\": base,\n",
    "                                \"val\": itertools.islice(\n",
    "                                            self.combinations,\n",
    "                                            partition,\n",
    "                                            successive,\n",
    "                                            1\n",
    "                                        ) \n",
    "                            }\n",
    "                        )\n",
    "            partition = successive\n",
    "            cc += 1\n",
    "        if successive < nck:\n",
    "            self.chunks.append(\n",
    "                            {\n",
    "                                \"num\": cc,\n",
    "                                \"len\": nck,\n",
    "                                \"val\": itertools.islice(\n",
    "                                            self.combinations,\n",
    "                                            successive,\n",
    "                                            nck,\n",
    "                                            1\n",
    "                                        ) \n",
    "                            }\n",
    "                        )\n",
    "        pprint.pprint(self.chunks)\n",
    "    def __call__(self):\n",
    "        try:\n",
    "            if not (os.path.exists(os.path.join(self.rawdir, self.rawfile))):\n",
    "                self.__download__()\n",
    "            if not (os.path.exists(os.path.join(self.outdir, self.outfile))):\n",
    "                self.__persist__()\n",
    "            print(\"Loading HD5\")\n",
    "            self.df     = dd.read_hdf(\n",
    "                os.path.join(self.outdir, self.outfile),\n",
    "                key=f'{self.outfile.split(\".\")[0]}'\n",
    "            ).compute()\n",
    "            self.df = self.df.head(10000)\n",
    "            self.__process__()\n",
    "            return self.array, self.names, self.chunks\n",
    "        except KeyboardInterrupt:\n",
    "            for name in dir():\n",
    "                if not name.startswith('_'):\n",
    "                    if name in globals():\n",
    "                        del globals()[name]\n",
    "                    else:\n",
    "                        del locals()[name]\n",
    "            gc.collect()\n",
    "            WeirdTargetsException(\"Keyboard Interrupt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HD5\n",
      "[                                        ] | 0% Completed |  0.9s"
     ]
    }
   ],
   "source": [
    "big_targets2 = BigTargets2(\n",
    "    './content/datasets',\n",
    "    'https://storage.googleapis.com/open-targets-data-releases/17.12/17.12_evidence_data.json.gz',\n",
    "    {\n",
    "        \"target\" : [\"target\", \"id\"],\n",
    "        \"disease\": [\"disease\", \"id\"],\n",
    "        \"score\":   [\"scores\", \"association_score\"]\n",
    "    }, \n",
    "    './content/results',\n",
    "    'tds.h5',\n",
    "    ['target', 'disease'],\n",
    "    './content/results',\n",
    "    'result.memmap'\n",
    ")\n",
    "array, names, chunks = big_targets2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCBC:\n",
    "    def __init__(self, array, names, chunks):\n",
    "        self.array  = array\n",
    "        self.names  = names\n",
    "        self.chunks = chunks\n",
    "        self.counter = 0\n",
    "    def __worker__(self, chunk):\n",
    "        local = 0\n",
    "        num = chunk[\"num\"]\n",
    "        with tqdm(desc=f'pool {num}', total=chunk['len'], leave=True, file=sys.stdout, position=0) as tqdmo:\n",
    "            for combination in chunk['val']:\n",
    "                common = self.array[combination[0]] & self.array[combination[1]]\n",
    "                if len(common) >= 2:\n",
    "                    local += 1\n",
    "                tqdmo.update(1)\n",
    "        return local\n",
    "    def __parallel__(self):\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            running_tasks = executor.map(self.__worker__, self.chunks)\n",
    "            for running_task in running_tasks:\n",
    "                self.counter += running_task\n",
    "    def __call__(self):\n",
    "        self.__parallel__()\n",
    "        print(self.counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcbc = PCBC(array, names, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pcbc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
